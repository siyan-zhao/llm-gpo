<!DOCTYPE html>
<html>
<head>
  <style>
  .container.is-max-desktop {
    max-width: 60%; /* or any other percentage or fixed value you prefer */
  }
  </style>
  <meta charset="utf-8">
  <meta name="description"
        content="Group Preference Optimization: Few-Shot Alignment of Large Language Models.">
  <meta name="keywords" content="Groue Preference Optimization, GPO, LLM, Large Language Models">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Group Preference Optimization: Few-Shot Alignment of Large Language Models</title>


</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>

</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Group Preference Optimization: Few-Shot Alignment of Large Language Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://google.com">Siyan Zhao</a><sup>1</sup>,
              <a href="https://google.com">John Dang</a><sup>1</sup>,
              <a href="https://google.com">Aditya Grover</a><sup>1</sup>,
            </span>
           
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of California, Los Angeles</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>

            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Many applications of large language models (LLMs), ranging from chatbots to creative writing, require nuanced subjective judgments that can differ significantly across different groups.
Existing alignment algorithms can be expensive to align for each group, requiring prohibitive amounts of group-specific preference data and computation for real-world use cases. 
          </p>
          <p>
             <center>This brings forth the key question studied in this work: </center>
 <center><b><i>How do we efficiently adapt LLMs to align closely with the opinions of specific interest groups?</i> </center></b>
          </p>
          
          <p>
            We introduce Group Preference Optimization (GPO), an alignment framework that steers language models to preferences of individual groups in a few-shot manner.
In GPO, we augment the base LLM with an independent transformer module trained to predict the preferences of a group for the LLM generations.
For few-shot learning, we parameterize this module as an in-context autoregressive transformer and train it via meta-learning on several groups. 
          </p>
          <p>
           We empirically validate the efficacy of GPO through rigorous evaluations using LLMs with varied sizes on three human opinion adaptation tasks. 
            These tasks involve adapting to the preferences of US demographic groups, global countries, and individual users. Our results demonstrate that GPO not only aligns models more accurately but also requires fewer group-specific preferences, 
            and less training and inference computing resources, outperforming existing strategies such as in-context steering and fine-tuning methods.
          </p>
          <img src="static/gpo_1.gif" alt="gpo"  width="1250" />

          <!-- <figure>
            <img src="static/GPO.png" alt="GPO" style="width:100%">
          </figure> -->
          <h2 class="title is-3">Background</h2>
          LLM may disproportionately over-represent some groups and under-represent others. Although guiding an LLM using prompts such as "Speak as xxx" can enhance its performance, this approach is limited and can sometimes lead to misrepresentation.
          <figure>
            <img src="static/gpo_background.png" alt="qualitative example for climate question" style="width:100%">
          </figure>
          
          
          While prior methods including supervised fine-tuning, prompting, and context learning have been useful in many alignment settings, these methods could perform poorly and be computationally expensive for aligning LLM outputs to group preferences, especially in low data regimes.

          
          
          
<hr>
          <h2 class="title is-3">Method</h2>
          <p>
            GPO performs few-shot supervised learning of human preference scores over LLM outputs. These predicted group preference scores can be utilized within downstream LLM alignment methods such as best-of-n sampling or PPO.
          </p>


           <figure>
            <img src="static/gpo_transformer.png" alt="qualitative example for climate question" style="width:100%">
          </figure>

          <p>
            GPO parameterizes a novel transformer module which in-context learns to predict group preference scores for new LLM outputs given a few examples of LLM outputs and ground-truth preference scores from that group.
          </p>

         
           <figure>
            <img src="static/algo.png" alt="qualitative example for climate question" style="width:70%">
          </figure>

<hr>
          <h2 class="title is-3">Results</h2>
 <h2 class="title is-4">Adapt to Group Preferences</h2>
          <p>
            GPO-aligned LLM outputs exhibit higher alignment scores compared with common baseline methods for multiple popular open-source models across varying parameter and pretraining dataset size scales, and opinion datasets.

          </p>

          <figure>
            <img src="static/alignment_results.png" alt="qualitative example for climate question" style="width:100%">
          </figure>
<hr>
          <p>
<p>
    Qualitatively, we find that GPO-aligned LLM output distributions are significantly more similar to group ground truth distributions than baseline methods.
</p>
<p>
    In the qualitative example below, the first row depicts the ground truth group opinion distribution. With limited context samples, GPO successfully adapts to match the opinion distributions of different groups. 
    For instance, it increases preference for option A when adapted to the group <em>Hindus</em>, whereas other steered LMs do not exhibit correct distribution changes. In particular, 
    <em>Llama2-13b-steered</em> seems biased towards a specific option, overemphasizing it instead of providing an accurate reflection of the targeted group's distribution.
</p>
<p>
    In contrast, for demographics like <em>College graduate/some postgrad</em> that have a balanced opinion distribution, GPO preserves this balance effectively. This underlines 
    GPO's ability not just to align with broad dataset group preferences, but also to fine-tune its alignment to specific groups with limited context.
</p>

          </p>

          <figure>
            <img src="static/climate.png" alt="qualitative example for climate question" style="width:120%">
          </figure>

         
<hr>

          <p>
            <b>Scalability with Increasing Context Samples.</b>
            GPO is also significantly more <b>sample efficient</b> than baseline methods, achieving higher alignment scores while using fewer examples.
          </p>


          <figure>
            <img src="static/efficiency.png" alt="qualitative example for climate question" style="width:70%">
          </figure>
          <hr>
 <h2 class="title is-4">Adapt to Individual Preferences</h2>
          <p>
            We also show that GPO is also able to adapt to a single individualâ€™s preferences, a setting where there is often much higher variance between preference datasets than the group preference alignment setting.
          </p>

          <figure>
            <img src="static/individual.png" alt="qualitative example for climate question" style="width:100%">
          </figure>
<hr>
          <h2 class="title is-3">Discussion and Future Work</h2>

          <p>
            GPO provides a framework for few-shot alligning LLMs to group preferences. GPO significantly outperforms prior
            methods as measured by alignment score for group preference alignment while requiring no gradient
            updates to the base LLM. We find that GPO is also more sample efficient, improving alignment score
            significantly more than baseline methods while using fewer samples, and is effective across multiple
            popular open-source LLMs of various parameter and pre-training dataset scales. Future work should explore adapting GPO 
            with other datasets (especially non-mulitiple choice format), the impact of aligning to group preferences on 
            alignment for other values including harmlessness or helpfulness, and using pre-training model initializations for 
            the GPO module
          </p>
        </div>

        
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- <div class="is-centered">
      
    </div> -->
    <!-- <div class=".iframe-container">
      
    

    </div> -->

  </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
